# Capstone Project: Store Sales Forecasting Dataset
import pandas as pd
import scipy
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
## Milestone 1: Data Understanding and Processing
### 1.1 Loading the Dataset and Displaying the Rows
# Attempt to read the CSV file with a fallback encoding
try:
    df = pd.read_csv('stores_sales_forecasting.csv', encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv('stores_sales_forecasting.csv', encoding='latin1')  # Fallback to 'latin1'

# Display the first few rows of the DataFrame
print(df.head())
### 1.2 Displaying Dataset Information
df.info()
### 1.3 Displaying Basic Statistics of Numerical Columns
df.describe()
### 1.4 Identifying Missing Data in the Dataset
df.isnull().sum()
### 1.5 Encoding Categorical Columns
# List of categorical columns
categorical_columns = ['Ship Mode', 'Segment', 'Country', 'City', 'State', 
                       'Region', 'Category', 'Sub-Category']
# Encoding categorical columns
# Use Label Encoding for simplicity
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])  
    label_encoders[col] = le  # Save encoders for later use
### 1.6 Feature Engineering
# Convert date columns to datetime format
df['Order Date'] = pd.to_datetime(df['Order Date'])
df['Ship Date'] = pd.to_datetime(df['Ship Date'])

# Extract time-based features
df['Order Year'] = df['Order Date'].dt.year
df['Order Month'] = df['Order Date'].dt.month
df['Order Day'] = df['Order Date'].dt.day
df['Order Quarter'] = df['Order Date'].dt.quarter
df['Ship Weekday'] = df['Ship Date'].dt.weekday

# Display the updated dataset with new features
df[['Order Date', 'Ship Date', 'Order Year', 'Order Month', 'Order Day', 'Order Quarter', 'Ship Weekday']].head()
### 1.7 Data Normalization
from sklearn.preprocessing import MinMaxScaler
from IPython.display import display

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Select numerical columns to normalize
numerical_cols = ['Sales', 'Discount', 'Profit', 'Quantity']  # Replace with actual numerical columns in your dataset

# Apply MinMaxScaler
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Print the normalized data with styled visualization
print("Normalized Data:")

# Function to apply a single background color
def single_color_background(val):
    return 'background-color: #E6E6FA;' # Light purple color code
# Styled DataFrame with a single light purple color and only the first 10 rows
styled_df = df.head(10).style.applymap(single_color_background).format(precision=3) 
display(styled_df)
### 1.8 Outlier Detection
# Function to detect and visualize outliers using boxplots
def detect_outliers(data, numerical_columns):
    for column in numerical_columns:
        plt.figure(figsize=(25, 5))
        sns.boxplot(x=data[column], color='#B19CD9')  # Set the boxplot color to #B19CD9
        plt.title(f'Outliers in {column}', fontsize=16)
        plt.xlabel(column, fontsize=12)
        plt.show()

# Numerical columns to check for outliers
numerical_columns = ['Sales', 'Quantity', 'Discount', 'Profit']

# Call the function to visualize outliers
detect_outliers(df, numerical_columns)
### 1.9 Remove Outliers
# Function to dynamically clean extreme outliers for each column
def clean_outliers_per_column(data, column, lower_bound=0.01, upper_bound=0.99):
    """
    Removes extreme outliers for a specific column using dynamic quantile thresholds.
    Parameters:
        data (DataFrame): The DataFrame containing the data.
        column (str): The column name for which to remove outliers.
        lower_bound (float): The lower quantile threshold (default: 0.01).
        upper_bound (float): The upper quantile threshold (default: 0.99).
    Returns:
        DataFrame: The DataFrame with extreme outliers removed for the column.
    """
    lower_quantile = data[column].quantile(lower_bound)
    upper_quantile = data[column].quantile(upper_bound)
    return data[(data[column] >= lower_quantile) & (data[column] <= upper_quantile)]

# List of numerical columns to clean
numerical_columns = ['Sales', 'Quantity', 'Discount', 'Profit']

# Remove outliers for each column while retaining as much data as possible
def clean_outliers_all_columns(data, columns, lower_bounds, upper_bounds):
    """
    Cleans outliers for all specified columns.
    Parameters:
        data (DataFrame): The DataFrame containing the data.
        columns (list): List of column names to clean.
        lower_bounds (dict): Dictionary specifying lower quantile thresholds for each column.
        upper_bounds (dict): Dictionary specifying upper quantile thresholds for each column.
    Returns:
        DataFrame: The cleaned DataFrame.
    """
    cleaned_data = data.copy()
    for column in columns:
        cleaned_data = clean_outliers_per_column(
            cleaned_data, column, lower_bounds[column], upper_bounds[column]
        )
    return cleaned_data

# Define dynamic quantile thresholds for each column
lower_bounds = {
    'Sales': 0.05,
    'Quantity': 0.05,
    'Discount': 0.01,
    'Profit': 0.01,
}

upper_bounds = {
    'Sales': 0.95,
    'Quantity': 0.95,
    'Discount': 0.99,
    'Profit': 0.99,
}

# Apply the tailored cleaning for each variable
cleaned_data = clean_outliers_all_columns(df, numerical_columns, lower_bounds, upper_bounds)
### 1.10 Visualizing of Outlier Removal: Before and After 
# Function to visualize cleaned data with boxplots and light purple background
def visualize_cleaned_data(original_data, cleaned_data, columns):
    for column in columns:
        plt.figure(figsize=(15, 5))
        plt.gcf().set_facecolor('#E6E6FA')  # Light purple background for the entire figure

        # Original data boxplot
        plt.subplot(1, 2, 1)
        sns.boxplot(x=original_data[column], color='#B19CD9')  # Light purple for the boxplot
        plt.title(f'Original Data - {column}')

        # Cleaned data boxplot
        plt.subplot(1, 2, 2)
        sns.boxplot(x=cleaned_data[column], color='#B19CD9')  # Light purple for the boxplot
        plt.title(f'Cleaned Data - {column}')

        plt.tight_layout()
        plt.show()

# Example usage of the function
# Assuming `df`, `cleaned_data`, and `numerical_columns` are defined
visualize_cleaned_data(df, cleaned_data, numerical_columns)
### 1.11 Train-Test Split for Modeling
from sklearn.model_selection import train_test_split

X = cleaned_data.drop(columns=['Row ID', 'Order Date','Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Product ID', 'Product Name', 'Sales'])
y= cleaned_data['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shape of the training and testing sets
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")
## Milestone 2: Exploratory Data Analysis (EDA)
### 2.1 Correlation: Statistical Relationship for Numerical Data
# Correlation matrix
correlation_matrix = cleaned_data[['Sales', 'Quantity', 'Discount', 'Profit']].corr()
# Heatmap for correlations
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='Purples', linewidths=0.5)
plt.title('Correlation Matrix', color='purple')
plt.show()
### 2.2 Identifying Trends
### 2.2.1 Visualize sales by region using a bar chart
plt.figure(figsize=(10, 6))
sns.barplot(
    x='Region',
    y='Sales',
    data=cleaned_data,
    estimator=sum,
    errorbar=None,  # Replace `ci=None` to avoid warnings
    palette=sns.color_palette("Purples", as_cmap=False) 
)
plt.title('Total Sales by Region', color='purple', fontsize=14)
plt.xlabel('Region', color='purple', fontsize=12)
plt.ylabel('Total Sales', color='purple', fontsize=12)
plt.xticks(color='purple', fontsize=10)
plt.yticks(color='purple', fontsize=10)
plt.show()
### 2.2.2 Visualize sales over time (monthly trends)
# Resample monthly sales data
monthly_sales = cleaned_data.resample('M', on='Order Date').sum()['Sales']

plt.figure(figsize=(12, 6))
monthly_sales.plot(color='purple', linewidth=2)  # Changed color to purple
plt.title('Monthly Sales Trend Over Time', color='purple', fontsize=14)
plt.xlabel('Month', color='purple', fontsize=12)
plt.ylabel('Total Sales', color='purple', fontsize=12)
plt.xticks(color='purple', fontsize=10)
plt.yticks(color='purple', fontsize=10)
plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)  # Keep grid subtle
plt.show()
segment_sales = cleaned_data.groupby('Segment')['Sales'].sum()

plt.figure(figsize=(8, 8))
segment_sales.plot.pie(
    autopct='%1.1f%%',
    colors=sns.color_palette("Purples", n_colors=len(segment_sales)),  # Use purple tones
    startangle=90
)
plt.title('Sales Distribution by Segment', color='purple', fontsize=14)
plt.ylabel('')  # Remove the y-axis label for cleaner pie chart
plt.show()
## Milestone 3: Model Development
pip install lightgbm
import lightgbm as lgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
import lightgbm as lgb
from lightgbm import early_stopping, log_evaluation
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
## 1. LightGBM Model
### 1.1 Initialize the LightGBM Model
model = lgb.LGBMRegressor(
    boosting_type='gbdt',
    objective='regression',
    metric='rmse',
    learning_rate=0.01,
    n_estimators=500,
    num_leaves=31
)
### 1.1.1 Train the Model with Early Stopping and Controlled Verbosity
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],  # Validation set
    eval_metric='rmse',
    callbacks=[
        early_stopping(stopping_rounds=10),  # Early stopping callback
        log_evaluation(10)  # Logs evaluation metrics every 10 rounds
    ]
)
###  1.1.2 Make Predictions
y_pred = model.predict(X_test)
### 1.1.3 Evaluate the Model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.6f}")
from sklearn.metrics import r2_score

# Assuming y_test and y_pred are defined
r2 = r2_score(y_test, y_pred)
print(f"R-squared (R²): {r2:.6f}")
## 1.2 Grid Search
# Initialize LightGBM model
model = lgb.LGBMRegressor()

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'n_estimators': [100, 200, 500]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2)
grid_search.fit(X_train, y_train)
### 1.2.1 Best Model and Predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(best_model)
### 1.2.2 Evaluate the Model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
# Assuming y_test and y_pred are defined
r2 = r2_score(y_test, y_pred)
print(f"R-squared (R²): {r2:.6f}")
## 2. LSTM Model 
### 2.1 Create Sequences (Sliding Window Approach)
def create_sequences(data, sequence_length):
    sequences = []
    targets = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i + sequence_length])
        targets.append(data[i + sequence_length])
    return np.array(sequences), np.array(targets)

sequence_length = 30  # Adjust as needed
X, y = create_sequences(cleaned_data['Sales'].values, sequence_length)
### 2.2 Split Into Training and Testing Sets
split_index = int(0.8 * len(X))  # 80-20 train-test split
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]
### 2.3 Reshape Inputs to Fit LSTM [Samples, Time Steps, Features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
### 2.4 Build LSTM Model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.3),
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    Dense(1)
])
### 2.5 Compile the Model
model.compile(optimizer='adam', loss='mean_squared_error')
### 2.6 Train the Model
history = model.fit(
    X_train, y_train, 
    epochs=20, 
    batch_size=32, 
    validation_data=(X_test, y_test),
    verbose=1
)
### 2.7 Make Predictions
predicted = model.predict(X_test)
### 2.8 Evaluate Predictions (e.g., Using Mean Squared Error)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, predicted)
print(f"Mean Squared Error: {mse}")
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.metrics import mean_absolute_error

# Ensure your data (X_train, y_train) is correctly shaped for LSTM
# X_train: Shape = (samples, timesteps, features)
# y_train: Shape = (samples,)

# Define the function to build the LSTM model
def build_lstm_model(units, optimizer):
    model = Sequential()
    model.add(LSTM(units=units, activation="tanh", input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer=optimizer, loss="mean_squared_error", metrics=["mae"])
    return model

# Define the hyperparameters grid
param_grid = {
    "units": [50, 100, 150],
    "batch_size": [16, 32],
    "epochs": [10, 20],
    "optimizer": ["adam", "rmsprop"],
}

# Manual grid search
best_params = None
best_score = float("inf")

for units in param_grid["units"]:
    for batch_size in param_grid["batch_size"]:
        for epochs in param_grid["epochs"]:
            for optimizer in param_grid["optimizer"]:
                # Convert optimizer name to optimizer object
                optimizer_obj = Adam() if optimizer == "adam" else RMSprop()
                
                # Build and train the model
                model = build_lstm_model(units=units, optimizer=optimizer_obj)
                history = model.fit(
                    X_train,
                    y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=0,
                    validation_split=0.2,
                )
                
                # Evaluate the model on validation data
                val_loss = np.min(history.history["val_loss"])
                if val_loss < best_score:
                    best_score = val_loss
                    best_params = {
                        "units": units,
                        "batch_size": batch_size,
                        "epochs": epochs,
                        "optimizer": optimizer,
                    }

print("Best Parameters:", best_params)
print("mse_improved_socre:", best_score)
